BDEPEND=python_targets_python3_10? ( >=dev-lang/python-3.10.11:3.10 ) >=dev-python/gpep517-13[python_targets_python3_10(-)?] >=dev-python/setuptools-67.7.2[python_targets_python3_10(-)?] >=dev-python/wheel-0.40.0[python_targets_python3_10(-)?]
DEFINED_PHASES=compile configure install prepare test
DESCRIPTION=Apache Spark Python API
EAPI=8
HOMEPAGE=https://github.com/apache/spark/tree/master/python
INHERIT=distutils-r1
IUSE=python_targets_python3_10
KEYWORDS=~amd64
LICENSE=Apache-2.0
RDEPEND=>=dev-java/spark-core-3.0.0 >=dev-python/pyarrow-1.0.0[python_targets_python3_10(-)?] >=dev-python/numpy-1.7[python_targets_python3_10(-)?] >=dev-python/pandas-0.23.2[python_targets_python3_10(-)?] >=dev-python/py4j-0.10.9[python_targets_python3_10(-)?] python_targets_python3_10? ( >=dev-lang/python-3.10.11:3.10 )
REQUIRED_USE=|| ( python_targets_python3_10 )
SLOT=0
SRC_URI=mirror://pypi/p/pyspark/pyspark-3.1.2.tar.gz
_eclasses_=out-of-source-utils	1a9007554652a6e627edbccb3c25a439	multibuild	bddcb51b74f4a76724ff7cf8e7388869	multilib	c19072c3cd7ac5cb21de013f7e9832e0	toolchain-funcs	513c31b3346458ed1f3878b57da6d61c	multiprocessing	b4e253ab22cef7b1085e9b67c7a3b730	ninja-utils	76050953ad5b70d7e09a6ca55558db92	python-utils-r1	a34d5f83235297b76d71eaf8deb53768	python-r1	3c6cd0f418ba702c186a9865b85e704d	distutils-r1	2d32e797ee29a8ffdd452f4a85860666
_md5_=6ff284009f93c4bf9450d070d74a8749
