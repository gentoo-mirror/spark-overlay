BDEPEND=python_targets_python3_10? ( >=dev-lang/python-3.10.9-r1:3.10 ) >=dev-python/gpep517-13[python_targets_python3_10(-)?] >=dev-python/setuptools-67.2.0[python_targets_python3_10(-)?] >=dev-python/wheel-0.38.4[python_targets_python3_10(-)?]
DEFINED_PHASES=compile configure install prepare test
DESCRIPTION=Apache Spark Python API
EAPI=8
HOMEPAGE=https://github.com/apache/spark/tree/master/python
INHERIT=distutils-r1
IUSE=python_targets_python3_10
KEYWORDS=~amd64
LICENSE=Apache-2.0
RDEPEND=>=dev-java/spark-core-3.0.0 >=dev-python/pyarrow-1.0.0[python_targets_python3_10(-)?] >=dev-python/numpy-1.7[python_targets_python3_10(-)?] >=dev-python/pandas-0.23.2[python_targets_python3_10(-)?] >=dev-python/py4j-0.10.9[python_targets_python3_10(-)?] python_targets_python3_10? ( >=dev-lang/python-3.10.9-r1:3.10 )
REQUIRED_USE=|| ( python_targets_python3_10 )
SLOT=0
SRC_URI=mirror://pypi/p/pyspark/pyspark-3.1.2.tar.gz
_eclasses_=out-of-source-utils	1a9007554652a6e627edbccb3c25a439	multibuild	bddcb51b74f4a76724ff7cf8e7388869	multilib	104e1332efb829e2f7cbf89307a474f1	toolchain-funcs	14a8ae365191b518fad51caad7a08f3e	multiprocessing	b4e253ab22cef7b1085e9b67c7a3b730	ninja-utils	76050953ad5b70d7e09a6ca55558db92	python-utils-r1	0857e21aa6e0f3748cfd8829a98ce1f7	python-r1	3c6cd0f418ba702c186a9865b85e704d	distutils-r1	a5ad17697c0eaba230f975a18abbac52
_md5_=6ff284009f93c4bf9450d070d74a8749
